{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene Expression Classification with Autoencoder and Classifier\n",
    "\n",
    "This notebook demonstrates a workflow for classifying gene expression data using an Autoencoder for dimensionality reduction followed by a simple neural network classifier. We'll generate synthetic gene expression data, train an Autoencoder to learn a lower-dimensional representation (latent space), and then train a classifier on these learned features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Cell 1: Imports\n",
    "\n",
    "We begin by importing all necessary libraries. These include `numpy` and `pandas` for data manipulation, `torch` for building and training neural networks, `matplotlib.pyplot` and `seaborn` for plotting, `sklearn.model_selection` for data splitting, `sklearn.preprocessing` for data scaling, `sklearn.metrics` for evaluation, `sklearn.decomposition` for PCA visualization, and `random` for setting seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Cell 2: Set Seeds for Reproducibility\n",
    "\n",
    "Setting random seeds ensures that our results are reproducible. This is crucial for debugging and comparing different model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Cell 3: Generate Synthetic Gene Expression Data\n",
    "\n",
    "We create a function `generate_synthetic_gene_data` to simulate gene expression data. This synthetic dataset will have `num_samples` (e.g., patients), `num_genes` (e.g., gene features), and `num_classes` (e.g., disease subtypes). The data is designed to have distinct gene expression patterns for each class to make the classification task feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_gene_data(num_samples=1000, num_genes=10000, num_classes=3):\n",
    "    X = np.random.normal(0, 1, size=(num_samples, num_genes))\n",
    "    y = np.random.randint(0, num_classes, size=(num_samples,))\n",
    "    for i in range(num_classes):\n",
    "        class_indices = np.where(y == i)[0]\n",
    "        X[class_indices, i*100:(i+1)*100] += np.random.normal(3, 0.5, size=(len(class_indices), 100))\n",
    "    return pd.DataFrame(X), pd.Series(y)\n",
    "\n",
    "X, y = generate_synthetic_gene_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Cell 4: Train-Test Split and Preprocessing\n",
    "\n",
    "We split the data into training and testing sets to evaluate our model's generalization performance. `stratify=y` ensures that the proportion of classes is maintained in both sets. `StandardScaler` is used to normalize the gene expression values, which is important for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Cell 5: Define Autoencoder\n",
    "\n",
    "An Autoencoder is a neural network trained to reconstruct its input. It consists of an **encoder** that maps the input to a lower-dimensional latent space and a **decoder** that reconstructs the input from this latent space. The `latent_dim` is a crucial hyperparameter, representing the dimensionality of the compressed representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512), nn.ReLU(),\n",
    "            nn.Linear(512, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed, latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Cell 6: Train Autoencoder\n",
    "\n",
    "The Autoencoder is trained using `MSELoss` (Mean Squared Error) as the criterion, aiming to minimize the difference between the input and its reconstruction. The Adam optimizer is used for updating the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(input_dim=10000, latent_dim=64)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    autoencoder.train()\n",
    "    reconstructed, _ = autoencoder(X_train_tensor)\n",
    "    loss = criterion(reconstructed, X_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Cell 7: Extract Latent Features\n",
    "\n",
    "After training, we use the encoder part of the Autoencoder to extract the learned latent features for both the training and testing datasets. These lower-dimensional features will be used as input for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _, X_train_latent = autoencoder(X_train_tensor)\n",
    "    _, X_test_latent = autoencoder(X_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Cell 8: Define Classifier\n",
    "\n",
    "We define a simple feed-forward neural network classifier that takes the `latent_dim` (64 in this case) as input and outputs scores for each of the `num_classes` (3 in this case). `CrossEntropyLoss` is suitable for multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Sequential(\n",
    "    nn.Linear(64, 32), nn.ReLU(),\n",
    "    nn.Linear(32, 3)\n",
    ")\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer_cls = optim.Adam(classifier.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Cell 9: Train Classifier\n",
    "\n",
    "The classifier is trained on the latent features extracted by the Autoencoder. The training process is similar to that of the Autoencoder, but here we optimize for classification accuracy using `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_cls = 10\n",
    "for epoch in range(epochs_cls):\n",
    "    classifier.train()\n",
    "    outputs = classifier(X_train_latent)\n",
    "    loss = criterion_cls(outputs, y_train_tensor)\n",
    "    optimizer_cls.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_cls.step()\n",
    "    print(f\"[Classifier] Epoch {epoch+1}/{epochs_cls}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Cell 10: Evaluate Classifier\n",
    "\n",
    "After training, we evaluate the classifier's performance on the unseen test set. We use `classification_report` to get a detailed breakdown of precision, recall, and F1-score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    preds = classifier(X_test_latent)\n",
    "    preds_label = torch.argmax(preds, dim=1)\n",
    "    print(classification_report(y_test_tensor, preds_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Cell 11: Confusion Matrix\n",
    "\n",
    "A confusion matrix provides a visual summary of the classification performance, showing the number of correct and incorrect predictions for each class. This helps in understanding which classes are being confused with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_tensor, preds_label)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Cell 12: Visualize Latent Space with PCA\n",
    "\n",
    "To understand the latent representation learned by the Autoencoder, we apply Principal Component Analysis (PCA) to reduce the 64-dimensional latent space to 2 dimensions. This allows us to visualize the clusters formed by different classes in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_test_latent.numpy())\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_test_tensor)\n",
    "plt.title(\"PCA of Latent Space\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}